{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a73d4a50-ce59-42b1-945d-f99658b4c155",
    "_uuid": "4cd2e240de9fb92725cd6931ac174d5c0ea197c6"
   },
   "source": [
    "# Predicting Breast Cancer - Logistic Regression\n",
    "\n",
    "# 0. Introduction\n",
    "---\n",
    "\n",
    "This notebook was inspired by [Mehgan Risdal's kernel](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) on the Titanic data, [Pedro Marcelino's kernel](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) on the Housing Prices data, and a very similar model by Mike M. Lee, also for breast cancer diagnosis prediction.\n",
    "\n",
    "The contents of this notebook will follow the outline below:\n",
    "\n",
    "[1. The Data](#the_data) - *Exploratory Data Analysis*\n",
    "\n",
    "[2. The Variables](#the_variables) - *Feature Selection*\n",
    "\n",
    "[3. The Model](#the_model) - *Building a Logistic Regression Model*\n",
    "\n",
    "[4. The Prediction](#the_prediction) - *Making Predictions with the Model*\n",
    "\n",
    "\n",
    "<a id='the_data'></a>\n",
    "# 1. The Data\n",
    "---\n",
    "*Data sourced from [UCI ML repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)*\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "* **id** \n",
    "* **diagnosis**: M = malignant, B = benign\n",
    "\n",
    "*Columns 3 to 32* \n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus: \n",
    "\n",
    "* **radius**: distances from center to points on the perimeter \n",
    "* **texture**: standard deviation of gray-scale values\n",
    "* **perimeter** \n",
    "* **area** \n",
    "* **smoothness**: local variation in radius lengths \n",
    "* **compactness**: perimeter^2 / area - 1.0 \n",
    "* **concavity**: severity of concave portions of the contour\n",
    "* **concave points**: number of concave portions of the contour\n",
    "* **symmetry** \n",
    "* **fractal dimension**: \"coastline approximation\" - 1\n",
    "\n",
    "The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.  For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "695ad59e-69fa-4a0d-a59f-204856d2c71e",
    "_kg_hide-output": true,
    "_uuid": "578c081220dce5fb90e3622f1ff3adefcef70971"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (812296560.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    conda install -c anaconda pandas\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "# data cleaning and manipulation \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import extra models to evaluate vs logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "# initialize some package settings\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd98c087-a305-4d9b-b581-c6245fdc8f25",
    "_kg_hide-output": false,
    "_uuid": "15f3de69cbef86cf97c2bf2c97ebe9faadf007ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in the data and check the first 5 rows\n",
    "df = pd.read_csv('data/data.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "175cd422-5cba-42de-95db-61746caffd4f",
    "_uuid": "08b46e9495442b7ea346ce9862e3b0f4ea60e8be"
   },
   "source": [
    "The last column, **Unnamed:32**, seems like it has a whole bunch of missing values. Let's quickly check for any missing values for other columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9a68601-5b5e-4e77-920d-f98e61b8505c",
    "_uuid": "acea2d29a01413419f5add3309727781e10da197",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# general summary of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "70cdb909-0a62-4114-99b9-d177b46c18fa",
    "_uuid": "283eb644ecf5581cb6c99baae6fd29c5e0444fd0"
   },
   "source": [
    "It looks like our data does not contain any missing values, except for our suspect column **Unnamed: 32**, which is full of missing values. Let's go ahead and remove this column entirely. After that, let's check for the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ba0851a-2cec-40fd-a0db-839f21b9fa62",
    "_kg_hide-output": true,
    "_uuid": "ac927ab4bbe80cc713a78109c725dbd67f81d342"
   },
   "outputs": [],
   "source": [
    "# remove the 'Unnamed: 32' column\n",
    "df = df.drop('Unnamed: 32', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80228e6c-863f-4205-b9c5-824f60cfc422",
    "_uuid": "9989c1a33c5b687d1d33ea6d7f4246f752278de7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the data type of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "724cdad7-4ffe-467b-a90a-2a7b6aa7ba02",
    "_uuid": "3db99e254faa194ec6c51d09671bb8a14b520e95"
   },
   "source": [
    "Our response variable, **diagnosis**, is categorical and has two classes,  'B' (Benign) and 'M' (Malignant). All explanatory variables are numerical, so we can skip data type conversion.\n",
    "\n",
    "Let's now take a closer look at our response variable, since it is the main focus of our analysis. We begin by checking out the distribution of its classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a8e4ba0-2e31-4c13-8fa5-8b80aff8bb28",
    "_uuid": "58b11df84464de83ff281f440dd0cfca16535cea"
   },
   "outputs": [],
   "source": [
    "# visualize distribution of classes \n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(df['diagnosis'], palette='RdBu')\n",
    "\n",
    "# count number of obvs in each class\n",
    "benign, malignant = df['diagnosis'].value_counts()\n",
    "print('Number of cells labeled Benign: ', benign)\n",
    "print('Number of cells labeled Malignant : ', malignant)\n",
    "print('')\n",
    "print('% of cells labeled Benign', round(benign / len(df) * 100, 2), '%')\n",
    "print('% of cells labeled Malignant', round(malignant / len(df) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95817caf-80c5-4256-ae93-1bc49a950b37",
    "_uuid": "2845c759a3614827bcd07417a57a601ebb6b32d8"
   },
   "source": [
    "Out of the 569 observations, 357 (or 62.7%) have been labeled malignant, while the rest 212 (or 37.3%) have been labeled benign. Later when we develop a predictive model and test it on unseen data, we should expect to see a similar proportion of labels.\n",
    "\n",
    "Although our dataset has 30 columns excluding the **id** and the **diagnosis** columns, they are all in fact very closely related since they all contain information on the same 10 key attributes but only differ in terms of their perspectives (i.e., the mean, standard errors, and the mean of the three largest values denoted as \"worst\"). \n",
    "\n",
    "In this sense, we could attempt to dig out some quick insights by analyzing the data in only one of the three perspectives. For instance, we could choose to check out the relationship between the 10 key attributes and the **diagnosis** variable by only choosing the \"mean\" columns.\n",
    "\n",
    "Let's quickly scan for any interesting patterns between our 10 \"mean\" columns and the response variable by generating a scatter plot matrix as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf6a891e-fd10-482e-a046-c8dbba28b668",
    "_uuid": "51dbbec5a09319c0dc1211745c1c9d5ebd725531"
   },
   "outputs": [],
   "source": [
    "# generate a scatter plot matrix with the \"mean\" columns\n",
    "cols = ['diagnosis',\n",
    "        'radius_mean', \n",
    "        'texture_mean', \n",
    "        'perimeter_mean', \n",
    "        'area_mean', \n",
    "        'smoothness_mean', \n",
    "        'compactness_mean', \n",
    "        'concavity_mean',\n",
    "        'concave points_mean', \n",
    "        'symmetry_mean', \n",
    "        'fractal_dimension_mean']\n",
    "\n",
    "sns.pairplot(data=df[cols], hue='diagnosis', palette='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20291b33-424f-4cf6-824a-dee11eb2c8dd",
    "_uuid": "9db39d34a3586db7b8b936b5c0de924d811f7319"
   },
   "source": [
    "There are some interesting patterns visible. For instance, the almost perfectly linear patterns between the **radius**, **perimeter** and **area** attributes are hinting at the presence of a correlation between these variables. Another set of variables that possibly imply correlation are the **concavity**, **concave_points** and **compactness**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "52ec8505-c1b2-41c6-aa2c-93f6ed64f425",
    "_uuid": "a14872dd172d43864cce34a4cc67ae5141289e64"
   },
   "source": [
    "<a id='the_variables'></a>\n",
    "# 2. The Variables\n",
    "---\n",
    "As said earlier, let's take a look at the correlations between our variables. This time however, we will create a correlation matrix with all variables (i.e., the \"mean\" columns, the \"standard errors\" columns, as well as the \"worst\" columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "73c26323-0332-4872-be87-2cffb9b1fdea",
    "_uuid": "d1b40364190bcacae69a2cf84c3f096665b0f845"
   },
   "outputs": [],
   "source": [
    "# Generate and visualize the correlation matrix\n",
    "corr = df.corr().round(2)\n",
    "\n",
    "# Mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set figure size\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Define custom colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0f69800-9d90-4f58-a884-d3d8230a06c4",
    "_uuid": "e3500469a85aa91872a1d3e9395a3036ffa569cb"
   },
   "source": [
    "Looking at the matrix, we can immediately verify the presence of correlation between some of our variables. For instance, the **radius_mean** column has a correlation of 1 and 0.99 with **perimeter_mean** and **area_mean** columns, respectively. This is probably because the three columns essentially contain the same information, which is the physical size of the observation (the cell). Therefore we should only pick one of the three columns when we go into further analysis. \n",
    "\n",
    "Another place where correlation is apparent is between the \"mean\" columns and the \"worst\" column. For instance, the **radius_mean** column has a correlation of 0.97 with the **radius_worst** column. In fact, each of the 10 key attributes display very high (from 0.7 up to 0.97) correlations between its \"mean\" and \"worst\" columns. This is somewhat inevitable, because the \"worst\" columns are essentially just a subset of the \"mean\" columns; the \"worst\" columns are also the \"mean\" of some values (the three largest values among all observations). Therefore, I think we should discard the \"worst\" columns from our analysis and only focus on the \"mean\" columns. \n",
    "\n",
    "In short, we will drop all \"worst\" columns from our dataset, then pick only one of the three attributes that describe the size of cells. But which one should be pick?\n",
    "\n",
    "Let's quickly go back to 6th grade and review some geometry. If we think of a cell as roughly taking a form of a circle, then the formula for its radius is, well, its radius,  *r*. The formulae for its perimeter and area are then **\\\\(2\\pi r\\\\) ** and **\\\\(\\pi r^2\\\\) **, respectively. As we can see, a cell's **radius** is the basic building block of its size. Therefore, I think it is reasonable to choose **radius** as our attribute to represent the size of a cell. \n",
    "\n",
    "Similarly, it seems like there is correlation between the attributes **compactness**, **concavity**, and **concave points**. Just like what we did with the size attributes, we should pick only one of these three attributes that contain information on the shape of the cell. I think **compactness** is an attribute name that is straightforward, so I will remove the other two attributes. \n",
    "\n",
    "We will now go head and drop all unnecessary columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "156ff174-a585-4eb6-9fbd-0c5861a6f439",
    "_uuid": "bac27000a23c99672fda30b7296c09fd3d67408e"
   },
   "outputs": [],
   "source": [
    "# first, drop all \"worst\" columns\n",
    "cols = ['radius_worst', \n",
    "        'texture_worst', \n",
    "        'perimeter_worst', \n",
    "        'area_worst', \n",
    "        'smoothness_worst', \n",
    "        'compactness_worst', \n",
    "        'concavity_worst',\n",
    "        'concave points_worst', \n",
    "        'symmetry_worst', \n",
    "        'fractal_dimension_worst']\n",
    "df = df.drop(cols, axis=1)\n",
    "\n",
    "# then, drop all columns related to the \"perimeter\" and \"area\" attributes\n",
    "cols = ['perimeter_mean',\n",
    "        'perimeter_se', \n",
    "        'area_mean', \n",
    "        'area_se']\n",
    "df = df.drop(cols, axis=1)\n",
    "\n",
    "# lastly, drop all columns related to the \"concavity\" and \"concave points\" attributes\n",
    "cols = ['concavity_mean',\n",
    "        'concavity_se', \n",
    "        'concave points_mean', \n",
    "        'concave points_se']\n",
    "df = df.drop(cols, axis=1)\n",
    "\n",
    "# verify remaining columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e54f22be-5cda-4509-8515-f670b1805ff7",
    "_uuid": "b45fb7fcf19dd29a5121d78d3ea51c0589987e1e"
   },
   "source": [
    "Are we all set now?\n",
    "\n",
    "Let's take a look at the correlation matrix once again, this time created with our trimmed-down set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a100b412-a601-4ef0-af1b-1a014328ec68",
    "_uuid": "95321bcfa9b6194fd49c3b5b9d9213e92d7796ce"
   },
   "outputs": [],
   "source": [
    "# Draw the heatmap again, with the new correlation matrix\n",
    "corr = df.corr().round(2)\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3bce675-cd9c-430b-a0ca-a4ce202520dc",
    "_uuid": "e4b91c2bd3fc2ba46acd748611f90bc9f74c8dfc"
   },
   "source": [
    "Looks great! Now let's move on to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea6a87e7-5192-4f3d-87ec-566e74b95c52",
    "_uuid": "fbc5ac65c21f0b0a7983bddfcc29c8ff7d4a325d"
   },
   "source": [
    "<a id='the_model'></a>\n",
    "# 3. The Model\n",
    "___\n",
    "\n",
    "It's finally time to develop our model! We will start by first splitting our dataset into two parts; one as a training set for the model, and the other as a test set to validate the predictions that the model will make.  We will set the test size to 0.3; i.e., 70% of the data will be assigned to the training set, and the remaining 30% will be used as a test set. In order to obtain consistent results, we will set the random state parameter to a value of 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "81c5e78d-c9e4-4909-9470-88fbbccafa32",
    "_uuid": "28b92554ae77ea3283734acdc12e6944bae8d9ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df\n",
    "y = df['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9450c803-b233-4cfb-ab2b-1a8a0275d636",
    "_uuid": "858b842d2a95ba099241c04c9f6c22d1f95b9814"
   },
   "source": [
    "Now that we have split our data into appropriate sets, let's write down the formula to be used for the `logistic regression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5b377ee-4462-4bd7-9d51-9a6d4512bc66",
    "_uuid": "4af1f9baaee26a15b3db2259ebf3a6611431f92d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a string for the formula\n",
    "cols = df.columns.drop('diagnosis')\n",
    "formula = 'diagnosis ~ ' + ' + '.join(cols)\n",
    "print(formula, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7bf632a2-5111-40d3-a6f7-34adfd3f30e3",
    "_uuid": "a3094a7abf18202fd0eba832fa6213f3582f0497"
   },
   "source": [
    "The formula includes all of the variables that were finally selected at the end of the previous section. We will now run the `logistic regression` with this formula and take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dbe16604-b13a-4c7c-bdf2-2e3ad0977f65",
    "_uuid": "90b8731f67bb6a8155cb44c97f17c53554f92336"
   },
   "outputs": [],
   "source": [
    "# Run the model and report the results\n",
    "model = smf.glm(formula=formula, data=X_train, family=sm.families.Binomial())\n",
    "logistic_fit = model.fit()\n",
    "\n",
    "print(logistic_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "46f7728a-079e-4925-bc48-84378713abdb",
    "_uuid": "dfe1d209f9b1b14458d114294245583853b37e3e"
   },
   "source": [
    "<a id='the_prediction'></a>\n",
    "# 4. The Prediction\n",
    "___\n",
    "\n",
    "In the previous section, we have successfully developed a logistic regression model. This model can take some unlabeled data and effectively assign each observation a probability ranging from 0 to 1. However, for us to evaluate whether the predictions are accurate, the predictions must be encoded so that each instance can be compared directly with the labels in the test data. So, instead of numbers between 0 or 1, the predictions should show \"M\" or \"B\", denoting malignant and benign respectively. In our model, a probability of 1 corresponds to the \"Benign\" class, whereas a probability of 0 corresponds to the \"Malignant\" class. Therefore, we can apply a threshhold value of 0.5 to our predictions, assigning all values closer to 0 a label of \"M\" and assigning all values closer to 1 a label of \"B\". \n",
    "\n",
    "Let's go through this step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b12d89a-417b-4e9e-8488-b4d72f3f2f58",
    "_uuid": "ad5ac6457c9f3389d3ef0545597d21e1dcf92475"
   },
   "outputs": [],
   "source": [
    "# predict the test data and show the first 5 predictions\n",
    "predictions = logistic_fit.predict(X_test)\n",
    "predictions[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "08edda96-9f39-4edc-943e-76dc7d9d6bd3",
    "_uuid": "837a08b171d11107ad1b5681906352692fa335d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note how the values are numerical. \n",
    "# Convert these probabilities into nominal values and check the first 5 predictions again.\n",
    "predictions_nominal = [ \"M\" if x < 0.5 else \"B\" for x in predictions]\n",
    "predictions_nominal[1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edda11cc-1a74-4bef-b69e-4bbbd6ebce71",
    "_uuid": "385a0e9890d2f22b6c6d5fe7456e20c4a5ee9e6f"
   },
   "source": [
    "We can confirm that probabilities closer to 0 have been labeled as \"M\", while the ones closer to 1 have been labeled as \"B\". Now we are able to evaluate the accuracy of our predictions by checking out the classification report and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7d1d4482-8f8a-44a5-b82b-6a6154201ec8",
    "_uuid": "6c9368e32cb571f6443283107b5936966aee4860"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions_nominal, digits=3))\n",
    "\n",
    "cfm = confusion_matrix(y_test, predictions_nominal)\n",
    "\n",
    "true_negative = cfm[0][0]\n",
    "false_positive = cfm[0][1]\n",
    "false_negative = cfm[1][0]\n",
    "true_positive = cfm[1][1]\n",
    "\n",
    "print('Confusion Matrix: \\n', cfm, '\\n')\n",
    "\n",
    "print('True Negative:', true_negative)\n",
    "print('False Positive:', false_positive)\n",
    "print('False Negative:', false_negative)\n",
    "print('True Positive:', true_positive)\n",
    "print('Correct Predictions', \n",
    "      round((true_negative + true_positive) / len(predictions_nominal) * 100, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e7d77a9d-1d67-411d-9bd3-dec7e7db7eeb",
    "_uuid": "510fac1f8a87e69999a5b7dcc56ca15368caa922"
   },
   "source": [
    "This model has an accuracy of 96.5% to correctly label the test data, there are several ways that we can increase this accuracy which might include using a different algorithm, or switch the variable set that we are using for the predictions but for the purpose of this exercise this should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
